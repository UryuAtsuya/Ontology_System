import streamlit as st
import os
import datetime
import json
from modules.ontology_manager import OntologyManager
from modules.llm_client import LLMClient
from modules.rag_engine import HybridRetriever

st.set_page_config(page_title="Seismic Ontology System", layout="wide")

# --- Sidebar: Config ---
with st.sidebar:
    st.header("System Config")
    default_key = st.secrets.get("GEMINI_API_KEY", "")
    api_key = st.text_input("Gemini API Key", value=default_key, type="password")
    uploaded_file = st.file_uploader("Upload OWL File", type=["owl", "rdf", "xml"])

# --- Initialization ---
if "manager" not in st.session_state:
    st.session_state["manager"] = OntologyManager()

mgr = st.session_state["manager"]

# Initialize LLM & Retriever if API Key is present
if api_key and "retriever" not in st.session_state:
    try:
        llm_client = LLMClient(api_key)
        retriever = HybridRetriever(mgr, llm_client)
        st.session_state["llm_client"] = llm_client
        st.session_state["retriever"] = retriever
    except Exception as e:
        st.error(f"Failed to initialize AI modules: {e}")

st.title("ğŸ—ï¸ Dynamic Seismic Ontology System")
st.markdown("SWRLãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãè‡ªå‹•åˆ†é¡ã¨ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰")

# --- ã‚¿ãƒ–æ§‹æˆ ---
tab_add, tab_reason, tab_visual, tab_ai = st.tabs(["â• å»ºç¯‰ç‰©ç™»éŒ²", "ğŸ§  æ¨è«–ãƒ»æ¤œè¨¼", "ğŸ“Š å¯è¦–åŒ–", "ğŸ¤– AI Assistant"])

# --- TAB 1: å»ºç¯‰ç‰©ç™»éŒ² ---
with tab_add:
    st.header("æ–°ã—ã„å»ºç¯‰ç‰©ã®ç™»éŒ²")
    st.info("ã“ã“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€SWRLãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚Šæ¨è«–ã‚¿ãƒ–ã§è‡ªå‹•çš„ã«ã€Œé«˜å±¤ã€ã‚„ã€Œè€éœ‡åŸºæº–ã€ãŒåˆ¤å®šã•ã‚Œã¾ã™ã€‚")

    if mgr.ontology:
        # --- Auto-Complete Section ---
        with st.expander("ğŸ“ è‡ªç„¶è¨€èªã‹ã‚‰è‡ªå‹•å…¥åŠ› (Auto-Complete)", expanded=False):
            raw_text = st.text_area("å»ºç¯‰ç‰©ã®èª¬æ˜ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„", placeholder="ä¾‹: 2020å¹´ã«ç«£å·¥ã—ãŸã€é«˜ã•150mã®æ¨ªæµœã«ã‚ã‚‹é‰„éª¨é€ ã®ã‚ªãƒ•ã‚£ã‚¹ãƒ“ãƒ«ã€‚")
            if st.button("Extract & Auto-fill (AIè§£æ)"):
                if raw_text:
                    if "llm_client" in st.session_state:
                         llm = st.session_state["llm_client"]
                         
                         # Options for mapping
                         locs = mgr.get_individuals_of_type("éƒ½é“åºœçœŒ")
                         structs = mgr.get_individuals_of_type("æ§‹é€ ç¨®åˆ¥å€¤")
                         uses = mgr.get_individuals_of_type("ç”¨é€”å€¤")
                         techs = mgr.get_individuals_of_type("è€éœ‡æŠ€è¡“å€¤")
                         
                         options = {
                             "éƒ½é“åºœçœŒ": [i.name for i in locs],
                             "æ§‹é€ ç¨®åˆ¥": [i.label.first() if i.label else i.name for i in structs],
                             "ç”¨é€”": [i.name.replace("ç”¨é€”", "") for i in uses],
                             "è€éœ‡æŠ€è¡“": [i.label.first() if i.label else i.name for i in techs]
                         }
                         
                         with st.spinner("Parsing..."):
                             parsed = llm.parse_building_info(raw_text, options)
                             st.session_state["parsed_data"] = parsed
                             st.success("è§£æå®Œäº†ï¼ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒ ã«åæ˜ ã•ã‚Œã¾ã—ãŸã€‚")
                    else:
                        st.error("AIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # Get default values from parsed data
        p_data = st.session_state.get("parsed_data", {})
        
        with st.form("building_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                # åŸºæœ¬æƒ…å ±ã®å…¥åŠ›
                name = st.text_input("åç§° (å¿…é ˆ)", value=p_data.get("åç§°", ""), placeholder="ä¾‹: æ–°å®¿ãƒ‘ãƒ¼ã‚¯ã‚¿ãƒ¯ãƒ¼")
                year = st.number_input("å»ºç¯‰å¹´", min_value=1800, max_value=2100, value=int(p_data.get("å»ºç¯‰å¹´", 2024)))
                height = st.number_input("é«˜ã• (m)", min_value=0.0, value=float(p_data.get("é«˜ã•_m", 30.0)))
                floors = st.number_input("éšæ•°", min_value=1, value=int(p_data.get("éšæ•°", 5)))
            
            with col2:
                # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
                locs = mgr.get_individuals_of_type("éƒ½é“åºœçœŒ")
                loc_map = {i.name: i for i in locs}
                
                # Try to match index
                def get_idx(options, target):
                    if not target: return 0
                    try: return options.index(target) + 1
                    except: return 0
                
                sel_loc = st.selectbox("å ´æ‰€ã«ã‚ã‚‹", [""] + list(loc_map.keys()), index=get_idx(list(loc_map.keys()), p_data.get("å ´æ‰€ã«ã‚ã‚‹")))
                
                structs = mgr.get_individuals_of_type("æ§‹é€ ç¨®åˆ¥å€¤")
                struct_map = {i.label.first() if i.label else i.name : i for i in structs}
                sel_struct = st.selectbox("æ§‹é€ ç¨®åˆ¥ã‚’æŒã¤", [""] + list(struct_map.keys()), index=get_idx(list(struct_map.keys()), p_data.get("æ§‹é€ ç¨®åˆ¥ã‚’æŒã¤")))
                
                uses = mgr.get_individuals_of_type("ç”¨é€”å€¤")
                use_map = {i.name.replace("ç”¨é€”", "") : i for i in uses}
                sel_use = st.selectbox("ç”¨é€”ã‚’æŒã¤", [""] + list(use_map.keys()), index=get_idx(list(use_map.keys()), p_data.get("ç”¨é€”ã‚’æŒã¤")))
                
                techs = mgr.get_individuals_of_type("è€éœ‡æŠ€è¡“å€¤")
                tech_map = {i.label.first() if i.label else i.name : i for i in techs}
                sel_tech = st.selectbox("è€éœ‡æŠ€è¡“ã‚’æŒã¤", [""] + list(tech_map.keys()), index=get_idx(list(tech_map.keys()), p_data.get("è€éœ‡æŠ€è¡“ã‚’æŒã¤")))

            submit = st.form_submit_button("Ontologyã«è¿½åŠ ")
            
            if submit and name:
                attrs = {
                    "åç§°": name,
                    "å»ºç¯‰å¹´": int(year),
                    "é«˜ã•_m": float(height),
                    "éšæ•°": int(floors),
                    "å ´æ‰€ã«ã‚ã‚‹": loc_map.get(sel_loc),
                    "æ§‹é€ ç¨®åˆ¥ã‚’æŒã¤": struct_map.get(sel_struct),
                    "ç”¨é€”ã‚’æŒã¤": use_map.get(sel_use),
                    "è€éœ‡æŠ€è¡“ã‚’æŒã¤": tech_map.get(sel_tech)
                }
                
                new_b = mgr.add_building(name, attrs)
                if new_b:
                    mgr.save_ontology()
                    st.success(f"ã€Œ{name}ã€ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
                    st.session_state["last_added"] = new_b
                else:
                    st.error("è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# --- TAB 2: æ¨è«–ãƒ»æ¤œè¨¼ ---
with tab_reason:
    st.header("SWRLæ¨è«–ã®å®Ÿè¡Œ")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        if st.button("æ¨è«–ã‚’å®Ÿè¡Œ (Pellet Reasoner)"):
            with st.spinner("æ¨è«–ä¸­..."):
                res = mgr.run_reasoner()
                
                if isinstance(res, dict) and res.get("status") == "Success":
                    st.success(res["message"])
                    st.session_state["last_inference_report"] = res.get("report", [])
                elif isinstance(res, str): # Fallback for old version
                    st.success(res)
                else:
                    st.error(res.get("message", "Error"))
        
        # çµæœä¿å­˜ãƒœã‚¿ãƒ³
        if st.button("æ¨è«–è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜"):
             # ... (simplified save logic)
             pass

    with col2:
        st.subheader("æ¨è«–çµæœã®èª¬æ˜ (Explainability)")
        report = st.session_state.get("last_inference_report", [])
        if report:
             for item in report:
                with st.expander(f"ğŸ—ï¸ {item['name']} (Classes: {', '.join(item['classes'])})"):
                    if item["explanations"]:
                        st.markdown("#### ğŸ“ Why?")
                        for cls, rules in item["explanations"].items():
                            st.markdown(f"**Classified as `{cls}` because:**")
                            for r in rules:
                                st.code(r, language="text")
                    else:
                        st.info("No specific SWRL rules triggered.")
        else:
            st.info("æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã“ã“ã«çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

# --- TAB 3: å¯è¦–åŒ– ---
# (Previous code assumed here, just keeping it minimal for simplicity in this replacement)
# Re-implementing correctly
# --- å…±é€š: å»ºç¯‰ç‰©ãƒªã‚¹ãƒˆã®å–å¾— ---
buildings = mgr.get_individuals_of_type("å»ºç¯‰ç‰©")
b_names = [b.name for b in buildings] if buildings else []

if "selected_building" not in st.session_state:
    st.session_state["selected_building"] = b_names[0] if b_names else None

with tab_visual:
    st.header("ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•å¯è¦–åŒ–")
    if b_names:
        target_b_name_vis = st.selectbox("å¯è¦–åŒ–ã™ã‚‹å»ºç¯‰ç‰©", b_names, key="vis_sel")
        if target_b_name_vis:
            target_b = mgr.ontology.search_one(iri=f"*{target_b_name_vis}")
            if target_b:
                graph = mgr.visualize_building(target_b)
                st.graphviz_chart(graph)
    else:
        st.info("No buildings found.")

# --- TAB 4: AI Assistant (Vector Search) ---
with tab_ai:
    st.header("Ontology-Guided AI Chat")
    if not api_key:
        st.warning("Please enter Gemini API Key in the sidebar.")
    else:
        if "chat_messages" not in st.session_state:
            st.session_state["chat_messages"] = []

        for msg in st.session_state["chat_messages"]:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
            st.session_state["chat_messages"].append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):
                    retriever = st.session_state.get("retriever")
                    llm = st.session_state.get("llm_client")
                    
                    if retriever and llm:
                        context_items = retriever.semantic_search(prompt)
                        context_str = retriever.format_context_for_llm(context_items)
                        # Pass context_items to enable Dynamic Few-Shot generation
                        response_text = llm.generate_response(prompt, context_str, retrieved_items=context_items)
                    else:
                        response_text = "AI modules not initialized. Please check API Key."
                    
                    st.markdown(response_text)
                    st.session_state["chat_messages"].append({"role": "assistant", "content": response_text})