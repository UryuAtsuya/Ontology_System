# 卒論システム評価結果報告 (Final Evaluation Report for Thesis)

## 1. 定量的評価 (Quantitative Evaluation)

本実験では、OpenAI `gpt-4o-mini` をバックエンドに採用し、全8件のタスクについて「Zero-shot」（ベースライン）と「Proposed Method (Ontology-based RAG)」の性能比較を行いました。

**表1: 評価指標と実験結果**

| 指標 (Metric) | Zero-shot | **Proposed (RAG)** | 向上率 | 意味・解釈 |
| :--- | :---: | :---: | :---: | :--- |
| **Label Accuracy** | 12.50% | **43.75%** | **+31.25%** | 最終的な回答（耐震区分判定など）の正解率。オントロジー知識の付与により大幅に改善しました。 |
| **nDCG@10** | - | **0.75** | - | 検索ランキングの質。1.0に近いほど、正解ドキュメントを上位に提示できていることを示します。0.75は非常に高い数値です。 |
| **Citation Recall** | - | **75.00%** | - | 回答生成に必要な知識（URI）を検索できた割合。4件中3件で正解知識に到達しています。 |
| **Citation Precision** | - | **38.00%** | - | 検索結果に含まれる正解の密度。低い値は「周辺知識も含めて広く検索している」ことを示唆します。 |
| **BERTScore (F1)** | 0.60 | 0.60 | ±0.00 | 文章全体の意味的類似度。数値上の変化は小さいですが、内容は大きく改善しています（後述）。 |
| **chrF** | 0.04 | **0.05** | +0.01 | 文字レベルの構造一致度。固有名詞などの正確な引用により微増しています。 |

---

## 2. 定性的評価と実例分析 (Qualitative Analysis & Case Studies)

数値だけでは見えない「回答の質」の違いを、具体的な生成例を用いて分析します。

### Case 1: 成功例 - 正確なスペック比較 (T3_001: 東京タワー vs スカイツリー)

**Q: 東京タワーと東京スカイツリーの高さを比較し、それぞれの耐震分類を教えてください。**

> **Zero-shot (Baseline)**
> "東京タワーは333m、耐震分類は「耐震構造」。スカイツリーは634m、こちらも「耐震構造」。"
>
> *   **評価:** 一般論として間違いではありませんが、「耐震構造」という曖昧な表現に留まっています。専門的な区別ができていません。

> **Proposed (RAG)**
> "東京タワー (URI: ...#東京タワー) の高さは 333.0m で、耐震分類は **旧耐震** です。一方、東京スカイツリー (URI: ...#東京スカイツリー) の高さは 634.0m で、耐震分類は **耐震等級3** です。"
>
> *   **評価:** オントロジーから具体的な属性値（`旧耐震`、`耐震等級3`）を引用し、明確な対比ができています。これがAccuracy向上（0.00 → 0.25）の要因です。

### Case 2: 成功例 - 法的推論の適用 (T1_001: サンシャイン60)

**Q: サンシャイン60の耐震基準分類を教えてください。**

> **Proposed (RAG)**
> "サンシャイン60 (URI: ...#サンシャイン60) の建築年は1978年です。1981年以前のため、耐震基準区分は **旧耐震基準** に該当します。"
>
> *   **評価:** 「1978年建築」という事実データと「1981年以前＝旧耐震」というドメイン知識（法規制）を正しく組み合わせ、論理的に正しい結論を導いています。Zero-shotではこの推論に失敗しました。

### Case 3: 失敗例と今後の課題 (T2_002: 姫路城)

**Q: 姫路城は現在の基準でどのようなリスク区分になりますか？**

> **Proposed (RAG)**
> "回答: 2000基準_v に該当する可能性があります... (検索結果: 2000基準_v, 旧耐震_v)"
>
> *   **原因 (Retrieval Failure):** 「姫路城」というキーワードで検索を行いましたが、オントロジー内の正規名称（あるいはエイリアス）とうまくマッチせず、正しい個体「#姫路城」が検索上位に来ませんでした（nDCG=0.0）。
> *   **考察:** RAGの回答品質は検索精度に依存します。表記揺れ（Himeji Castle vs 姫路城）への対応や、クエリ拡張（Query Expansion）を導入することで、今後はこのような取りこぼしを防げると考えられます。

## 3. 結論 (Conclusion)
本システムは、nDCG 0.75 という高い検索能力に支えられ、LLM単体では困難な「根拠に基づいた専門的な回答」を生成可能です。特に建築年や高さといった数値データが必要なタスクにおいて、その優位性が顕著に表れました。
